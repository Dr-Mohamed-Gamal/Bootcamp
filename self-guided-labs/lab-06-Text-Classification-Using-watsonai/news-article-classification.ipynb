{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ea90db",
   "metadata": {},
   "source": [
    "## watsonx.ai challenge: news article Classification\n",
    "The following notebook uses zero-shot text classification of news articles.  However it was not written using Prompt Patterns and would also benefit from using a more effective few-shot learning technique to improve model accuracy.\n",
    "\n",
    "#### Apply what you've learned \n",
    "To complete the foundations of watsonx.ai technical series, we have provided a [notebook using generative AI to classify text in news articles](./news-article-classification.ipynb). Read through the notebook to understand how it approached the challenge of classifying text. Your mission is to rewrite the notebook by applying your new skills in prompt engineering to achieve a higher accuracy (already a high 90+%):\n",
    "\n",
    "#### Need Ideas?\n",
    "- Use the LangChain library's Prompt Template functionality?\n",
    "- Implement a better few-shot learning technique to improve model accuracy?\n",
    "- Perhaps be a rebel and show that a standard ML model performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd9ca08",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset is collection of more than 1 million gathered from more than 2000 news sources by ComeToMyHead over one year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research and educational purposes. For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html.\n",
    "\n",
    "Each news article was tagged with one of four financial classes:\n",
    "\n",
    "- world\n",
    "- science_technology\n",
    "- business\n",
    "- sports\n",
    "\n",
    "The dataset is broken into train and test sets of 120,000 and 7600 rows respectively. You will notice that only 2.5% of the dataset is used by default to limit the # of calls being made to watsonx.ai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8ff3b",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Step 1. Initialize system](#fm_initialize)\n",
    "* [Step 2. Select the Model and Set Parameters](#fm_params)\n",
    "* [Step 3. Prepare the Input Data](#fm_input)\n",
    "* [Step 4. Run the Model Iterating over a Sample of the Dataset](#fm_iterate)\n",
    "* [Step 5. Merge and Examine the Output Data](#fm_merge_examine)\n",
    "* [Step 6. Analyze the Results](#fm_analyze)\n",
    "* [Step 7. Submit Your Notebook for Review](#fm_submit_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38c929",
   "metadata": {},
   "source": [
    "<a id=\"fm_initialize\"></a>\n",
    "# Step 1. Initialize System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192300d9",
   "metadata": {},
   "source": [
    "### 1.1 Import libraries\n",
    "Import libraries and configure the system\n",
    "We import the required libraries and IBM Generative AI modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d2d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9960013",
   "metadata": {},
   "source": [
    "### 1.2 Import config file\n",
    "\n",
    "To support greater flexibility and configuration, this notebook uses a *config.json* file containing these options:\n",
    "- `input_file_path`: The file path for the input data file for analysis in a CSV format.\n",
    "- `output_file_path`: The file path for storing the classification data output.\n",
    "- `input_value`: The header name of the input sentences from the input data CSV file.\n",
    "- `original_class`: The header name of the original class from the input data CSV file.\n",
    "- `classifications`: An array containing all categories.\n",
    "- `misclassified_file_path`: The file path for storing the misclassified data.\n",
    "- `output_file_header`: An array of headers to specify the output file column order.\n",
    "- `sample_size`: The percentage of the input file to be sampled for processing.\n",
    "- `params`: These additional parameters specify the model behavior.\n",
    "\t- `decoding_method`: This specifies the strategy for selecting tokens during the generation of the output text. This parameter can take on values of \"greedy\" or \"sample\", with \"sample\" as the default value if not specified.\n",
    "\t- `max_new_tokens`: The maximum number of new tokens to be generated. This parameter ranges from 1 to 1024, with a default value 20.\n",
    "\t- `min_new_tokens`: If stop sequences are given, they are ignored until the minimum tokens are generated. The parameter defaults to 0.\n",
    "\t- `random_seed`: Random number generator seed to use in sampling mode for experimental repeatability. The *random_seed* Must be greater than or equal to one.\n",
    "\t- `stop_sequences`: Stop sequences are one or more strings that will cause the text generation to stop when they are produced in the output.\n",
    "\t- `temperature`: The *temperature* modifies the next-token probabilities when running in sampling mode.\n",
    "\t- `top_k`: The number of highest probability vocabulary tokens to retain for top-k-filtering.\n",
    "\t- `top_p`: This parameter is similar to top_k, except the candidates to generate the next token are the most likely tokens with probabilities that sum to at least top_p. \n",
    "\n",
    "See the [wml sdk documentation](https://ibm.github.io/watson-machine-learning-sdk/foundation_models.html) for more paramteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('config.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "params_mapping = {\n",
    "    'decoding_method':GenParams.DECODING_METHOD,\n",
    "    'max_new_tokens':GenParams.MAX_NEW_TOKENS,\n",
    "    'min_new_tokens':GenParams.MIN_NEW_TOKENS,\n",
    "    'random_seed':GenParams.RANDOM_SEED,\n",
    "    'stop_sequences':GenParams.STOP_SEQUENCES,\n",
    "    'temperature':GenParams.TEMPERATURE,\n",
    "    'top_k':GenParams.TOP_K,\n",
    "    'top_p':GenParams.TOP_P\n",
    "}\n",
    "\n",
    "params = {params_mapping[key]:val for key, val in  json_data['params'].items()}\n",
    "\n",
    "temp_path = json_data['temp_path']\n",
    "input_file_path_temp = os.path.join(temp_path,\"inputTemp.csv\")\n",
    "output_file_path_temp = os.path.join(temp_path,\"outputTemp.csv\")\n",
    "os.makedirs(temp_path, exist_ok=True)\n",
    "\n",
    "output_path = json_data['output_path']\n",
    "output_file_path = os.path.join(output_path,json_data['output_file'])\n",
    "os.makedirs(output_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abda61a",
   "metadata": {},
   "source": [
    "<a id=\"fm_params\"></a>\n",
    "# Step 2. Select the Model and Set Parameters\n",
    "In this step we select a model and set the parameters specific to the model to control the behavior.\n",
    "\n",
    "In the default example, the only parameter passed to the models is decoding_method=\"greedy\". When the *decoding_method* is set to \"greedy\", all generated text is included, so no additional parameters are needed.\n",
    "\n",
    "You could explore using decoding_method=\"sample\" and then set several additional parameters to control text generation. The *temperature* parameter adjusts the randomness of the model's output: setting it to 0.7 gives some randomness, but ensures the model isn't too random. The *min_new_tokens* and *max_new_tokens* parameters control the minimum and maximum number of new tokens the model can generate. The *top_k* and *top_p* parameters control how the model selects tokens from the predicted tokens, and are typically set to 50 and 1, respectively. With those settings, the model will choose the 50 most probable tokens and will not randomly select tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc4146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# Ensure you copied the .env file that you created earlier into the same directory as this notebook\n",
    "try:\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"API_KEY\")\n",
    "except Exception:\n",
    "    api_key = getpass.getpass(\"Please enter your api key (hit enter): \")\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "project_id = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }\n",
    "\n",
    "FLAN = Model(\n",
    "        model_id=\"google/flan-ul2\",\n",
    "        params=params,\n",
    "        credentials=creds,\n",
    "        project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee64dc",
   "metadata": {},
   "source": [
    "<a id=\"fm_input\"></a>\n",
    "# Step 3. Prepare the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de7e7be",
   "metadata": {},
   "source": [
    "## 3.1 Load configuration\n",
    "Read input and specify indices are first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08674381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_file_path=json_data['input_file_path']\n",
    "input_response = pd.read_csv(input_file_path, index_col=0)\n",
    "input_response.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415f78f",
   "metadata": {},
   "source": [
    "## 3.2 Provide instructions to the model\n",
    "Careful design of your instructions may seem easy to ignore but is a crucial step of Prompt Engineering.  As a test, try setting these instructions to an empty string \"\" to see how well the prompt performs without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b4670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide instructions to set context for the model\n",
    "instructions = \"Classify these news articles into one of these categories: \"\n",
    "instructions += ', '.join(json_data[\"classifications\"]) + \"\\n\\n\"\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029697a",
   "metadata": {},
   "source": [
    "## 3.3 Review example prompt\n",
    "We assemble one example of the prompt that will be passed to the LLM, consisting of the classification request combined with the text to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f273c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_prompt(input_value):\n",
    "    return instructions + input_value\n",
    "\n",
    "input_value_header=json_data[\"input_value\"]\n",
    "original_class_header=json_data[\"original_class\"]\n",
    "with open(input_file_path, \"r\") as input_file:\n",
    "    reader = csv.DictReader(input_file)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i == 0:\n",
    "            input_value = row[input_value_header]\n",
    "            original_class = row[original_class_header]\n",
    "            example_prompt = generate_prompt(input_value)\n",
    "            print(example_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63257630",
   "metadata": {},
   "source": [
    "<a id=\"fm_example_output\"></a>\n",
    "## 3.4 Review model response for prompt\n",
    "We assemble one example of the prompt that will be passed to the LLM, consisting of the classification request combined with the text to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb953380",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = FLAN.generate_text(example_prompt)\n",
    "model_class = response\n",
    "input_text = example_prompt\n",
    "input_text = input_text[len(instructions):len(input_text)]\n",
    "\n",
    "print(f\"The predicted class was '{model_class}'\")\n",
    "print(f\"The actual class was '{original_class}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537b215",
   "metadata": {},
   "source": [
    "<a id=\"fm_iterate\"></a>\n",
    "# Step 4. Run the Model Iterating over a Sample of the Dataset\n",
    "We now iterate through the entire dataset, constructing prompts for each input text entry  and submitting them to the LLM. The sample_percent parameter specifies the percentage of the input text entries that will be submitted to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(json_data['input_file_path'])\n",
    "original_dataset_size = len(original_df)\n",
    "sample_percent = json_data[\"sample_percent\"]\n",
    "sample_size = int(( sample_percent / 100) * original_dataset_size)\n",
    "print(f\"{sample_percent}% or {sample_size} of {original_dataset_size} samples will be evaluated\")\n",
    "\n",
    "df_sample = original_df.sample(n = sample_size, random_state=123456)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b393bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the input and output CSV files\n",
    "with open(input_file_path_temp, \"w\", newline=\"\") as input_file_temp, open(output_file_path_temp, \"w\", newline=\"\") as output_file:\n",
    "\n",
    "    # Create CSV reader and writer objects\n",
    "    writer_out = csv.DictWriter(output_file, fieldnames=json_data[\"output_file_header\"])\n",
    "    writer_temp = csv.DictWriter(input_file_temp, fieldnames=[\"original_class\", \"passage\"])\n",
    "\n",
    "    # Write the header row to the output CSV file\n",
    "    writer_out.writeheader()\n",
    "    writer_temp.writeheader()\n",
    "    \n",
    "    # Loop through each row in the input CSV file\n",
    "    prompts_to_complete = []\n",
    "    completed_prompts = []\n",
    "    print(\"creating prompts...\")\n",
    "    for index, row in df_sample.iterrows():\n",
    "        input_value = row[input_value_header]\n",
    "        original_class = row[original_class_header]\n",
    "        writer_temp.writerow({\"original_class\": original_class, \"passage\": input_value})\n",
    "        prompt = generate_prompt(input_value)\n",
    "        prompts_to_complete.append(prompt)\n",
    "            \n",
    "    num_of_prompts = len(prompts_to_complete)\n",
    "    print(f\"{num_of_prompts} prompts created\")\n",
    "\n",
    "    # generate async on all prompts\n",
    "    responses = [FLAN.generate_text(prompt) for prompt in prompts_to_complete]\n",
    "    for count, (input_text, classification) in enumerate(zip(prompts_to_complete, responses)):\n",
    "        input_text = input_text[len(instructions):len(input_text)]\n",
    "        writer_out.writerow({\"index\": count, \"predicted_class\": classification, \"passage\": input_text})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7ea4e-35f8-4802-a24e-4d521c11fa99",
   "metadata": {},
   "source": [
    "<a id=\"fm_merge_examine\"></a>\n",
    "# Step 5. Merge and Examine the Output Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152c39b",
   "metadata": {},
   "source": [
    "<a id=\"fm_merge_output\"></a>\n",
    "## 5.1 Merge output data\n",
    "We next merge the input data with the processed output to create a single output file for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f63bb-ad99-4606-afad-e5ef13d67e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first CSV file with original class and passage columns\n",
    "df_input_temp = pd.read_csv(input_file_path_temp)\n",
    "\n",
    "# Read the second CSV file with predicted class and passage columns\n",
    "df_output_temp = pd.read_csv(output_file_path_temp)\n",
    "\n",
    "# Merge the two dataframes based on the passage column\n",
    "merged_df = pd.merge(df_input_temp, df_output_temp, on='passage')\n",
    "merged_df = merged_df.drop_duplicates(subset='passage')\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f248c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"fm_examine_output\"></a>\n",
    "## 5.2 Examine the model predictions\n",
    "We can examine a small set of the combined data to compare the quality of the results. The data presented below shows the class (original sentiment) and the class predicted by the model (predicted sentiment). As can be seen, a majority of the classes in the small sample are predicted correctly and match. The combination of of the input and output data will allow us perform other analysis of the classification results. This is kept in memory as the *output_response* array for later access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_response = pd.read_csv(output_file_path, index_col=None)\n",
    "output_response.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c664ec4",
   "metadata": {},
   "source": [
    "<a id=\"fm_analyze\"></a>\n",
    "# Step 6. Analyze the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5059b",
   "metadata": {},
   "source": [
    "<a id=\"fm_compare\"></a>\n",
    "## 6.1 Comparison\n",
    "Compare the predicted and original categories to find the percentage that exactly match. A percentage of 100% indicates all predictions match the original classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7754dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open(output_file_path))\n",
    "test_dataset_size = len(list(reader))\n",
    "# Skip the header\n",
    "test_dataset_size = test_dataset_size - 1\n",
    "\n",
    "matched_rows = 0\n",
    "with open(output_file_path, 'r') as csvfile:\n",
    "    datareader = csv.reader(csvfile)\n",
    "    for row in datareader:\n",
    "\n",
    "        # get the values of the two columns to compare\n",
    "        column1 = row[0]\n",
    "        column2 = row[3]\n",
    "        if column1 == column2:\n",
    "            matched_rows+=1\n",
    "match_percent = (matched_rows / test_dataset_size) * 100\n",
    "print(f\"{match_percent:.2f}% rows matched\")\n",
    "print(f\"{matched_rows} of {test_dataset_size} rows matched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f1791e",
   "metadata": {},
   "source": [
    "<a id=\"fm_post_processing\"></a>\n",
    "## 6.2 Post-process the dataset in preparation for performance analysis\n",
    "\n",
    "Here, we post-process the dataset and generate sets of the original and predicted categories. We then create a file of the misclassified data that will be used in later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.unique(output_response['original_class']))\n",
    "print(pd.unique(output_response['predicted_class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78fb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_list=list(pd.unique(output_response['original_class']))\n",
    "attribute_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a470e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_value_counts={}\n",
    "predicted_value_counts={}\n",
    "for i in attribute_list:\n",
    "    original_value_counts['original_'+i]=output_response['original_class'].value_counts()[i]\n",
    "    predicted_value_counts['predicted_'+i]=output_response['predicted_class'].value_counts()[i]\n",
    "print('original_value_counts=',original_value_counts)\n",
    "\n",
    "print('\\predicted_value_counts=',predicted_value_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4790d5",
   "metadata": {},
   "source": [
    "<a id=\"fm_accuracy_data\"></a>\n",
    "## 6.3 Prepare to compute model accuracy\n",
    "We create a copy of the *output_response* array named *dff* and then create separate vectors of the original (*y_test*) and predicted (*y_pred*) categories for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff= output_response.copy()\n",
    "y_test=dff['original_class']\n",
    "y_pred=dff['predicted_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d1930",
   "metadata": {},
   "source": [
    "<a id=\"fm_accuracy\"></a>\n",
    "## 6.4 Compute model accuracy\n",
    "\n",
    "Comparing the original and predicted categories and displaying the accuracy score as computed by scikit-learn should give an accuracy value that matches what was computed in section 6.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "print('Accuracy of model ',accuracy_score(y_test,y_pred)*100,'%')\n",
    "#this value should match with Match percentage estimated in comparison cell above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f297fb",
   "metadata": {},
   "source": [
    "<a id=\"fm_confusion\"></a>\n",
    "## 6.5 Generate a confusion matrix\n",
    "Next we compute and display the confusion matrix that characterizes the correct and incorrect categorizations and displays them graphically. Correct matches appear along a top-left to bottom-right diagonal. Cells with higher numbers of entries are shown in yellow ranging across the spectrum to lower numbers of entries shown in purple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acdf14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from textwrap import wrap\n",
    "xy_label=['\\n'.join(attribute.split('_')) for attribute in  attribute_list]\n",
    "cm = confusion_matrix(y_test, y_pred,labels=attribute_list)\n",
    "# print('Confusion Matrix \\n',cm)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=xy_label)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "plt.title('Confusion Matrix')\n",
    "cm_display.plot(ax=ax)\n",
    "# plt.setp(ax.get_xticklabels(), rotation=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
