{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "# Use watsonx to tune Meta `llama-2-13b-chat` model with Consumer Financial Protection Bureau document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Disclaimers\n",
    "\n",
    "- Use only Projects and Spaces that are available in watsonx context.\n",
    "\n",
    "\n",
    "## Notebook content\n",
    "\n",
    "This notebook contains the steps and code to demonstrate support of prompt tuning in watsonx.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.10.\n",
    "\n",
    "\n",
    "## Learning goal\n",
    "\n",
    "The goal of this notebook is to demonstrate how to:\n",
    "- upload of dataset with prompts to container (project)\n",
    "- finding best tuning parameters (hyper-parameters optimization)\n",
    "- trigger prompt tuning process\n",
    "- review of prompt tuning process details/results\n",
    "- deploy prompt-tuned model/asset\n",
    "- evaluation (via inference) of prompt-tuned model/asset\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Data loading](#data_loading)\n",
    "- [Initialize experiment](#initialize)\n",
    "- [Hyper-parameters optimization](#hpo)\n",
    "- [Run Prompt Tuning](#run_tuning)\n",
    "- [Prompt Tuning details](#run_details)\n",
    "- [Deploy](#deploy)\n",
    "- [Foundation Models Inference](#models_inference)\n",
    "- [Analyze the satisfaction](#predict)\n",
    "- [Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install and import the `datasets` and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ibm-watsonx-ai | tail -n 1\n",
    "!pip install scikit-learn | tail -n 1\n",
    "!pip install matplotlib | tail -n 1\n",
    "!pip install wget | tail -n 1\n",
    "!pip install scikit-optimize | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Watsonx API connection\n",
    "This cell defines the credentials required to work with watsonx API for Foundation\n",
    "Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the project id\n",
    "The Foundation Model requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"API_KEY\")\n",
    "except Exception:\n",
    "    api_key = getpass.getpass(\"Please enter your api key (hit enter): \")\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "project_id = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create an instance of APIClient with authentication details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient\n",
    "\n",
    "client = APIClient(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To be able to interact with all resources available in Watson Machine Learning, you need to set **project_id** which you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client.set.default_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"data_loading\"></a>\n",
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This example uses the Consumer Financial Protection Bureau training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'cfpb_train.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Download training data from git repository and create data assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "url = \"https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/prompt_tuning/cfpb_train.json\"\n",
    "if not os.path.isfile(filename): \n",
    "    wget.download(url)\n",
    "    \n",
    "asset_details = client.data_assets.create(name=filename, file_path=filename)\n",
    "asset_id = client.data_assets.get_id(asset_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define connection information to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.helpers import DataConnection\n",
    "\n",
    "data_conn = DataConnection(data_asset_id=asset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"initialize\"></a>\n",
    "## Initialize experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.experiment import TuneExperiment\n",
    "\n",
    "experiment = TuneExperiment(creds, project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "All avaliable tasks are presented under `Tasks` Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "{task.name: task.value for task in experiment.Tasks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"hpo\"></a>\n",
    "## Hyper-parameters optimization (optional, can take a while to complete!)\n",
    "\n",
    "The goal of hyper-parameters optimization is to find the best `learning_rate` value yielding the lowest loss. The search space can be extended to other parameters like `num_epochs` or `batch_size`.\n",
    "**Hint:** To execute the code change the cell type to Code and remove markdown marks. Note that HPO takes significant time and resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train function\n",
    "Let's define the `train_evaluate` function that will accepts set of parameters from SEARCH dict and return the loss value.\n",
    "In addition, we will measure the elapsed time. To speed up the HPO time we will use only 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# def train_evaluate(search_params):\n",
    "#     prompt_tuner = experiment.prompt_tuner(name=\"Prompt tuning with HPO - \" + str(search_params),\n",
    "#                                            base_model='meta-llama/llama-2-13b-chat',\n",
    "#                                            task_id=experiment.Tasks.CLASSIFICATION,\n",
    "#                                            tuning_type=experiment.PromptTuningTypes.PT,\n",
    "#                                            num_epochs=3,\n",
    "#                                            batch_size=8,\n",
    "#                                            max_input_tokens=256,\n",
    "#                                            max_output_tokens=20,\n",
    "#                                            verbalizer=\"Including narratives choice the best match product with the items from the list: 'credit_card', 'debt_collection', 'mortgages_and_loans', 'credit_reporting', 'retail_banking'. Input: {{input}} Output: \",\n",
    "#                                            auto_update_model=False,\n",
    "#                                            **search_params\n",
    "#                                            )\n",
    "\n",
    "#     tuning_details = prompt_tuner.run(training_data_references=[data_conn], background_mode=True)\n",
    "#     print(f\"Starting the tune...\")\n",
    "#     print(f\"hyper-parameters\", str(search_params))\n",
    "#     start = time.time()\n",
    "\n",
    "#     while prompt_tuner.get_run_status() not in ['failed', 'canceled', 'completed']:\n",
    "#         tuning_details = prompt_tuner.get_run_details()\n",
    "#         time.sleep(10)\n",
    "#     end = time.time() - start\n",
    "\n",
    "#     if prompt_tuner.get_run_status() in ['failed', 'canceled']:\n",
    "#         print(\"Model tuning failed or halted\")\n",
    "#         print(f\"get_run_details: {prompt_tuner.get_run_details()}\")\n",
    "#         return 10\n",
    "\n",
    "#     try:\n",
    "#        loss = prompt_tuner.summary().loss.values[0]\n",
    "#     except:\n",
    "#         loss = 10\n",
    "\n",
    "#     print(prompt_tuner.get_run_status(), 'loss: ',  loss, ' | run time', end)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Search space and optimization\n",
    "In next step let's define the search space for `learning_rate` and objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "# import skopt\n",
    "\n",
    "# SPACE = [\n",
    "#     skopt.space.Real(0.001, 0.005, name='learning_rate', prior='log-uniform')\n",
    "# ]\n",
    "\n",
    "# @skopt.utils.use_named_args(SPACE)\n",
    "# def objective(**params):\n",
    "#     return train_evaluate(params)\n",
    "\n",
    "# results = skopt.forest_minimize(objective, SPACE, n_calls=10)\n",
    "# best_loss = results.fun\n",
    "# best_params = results.x\n",
    "# # best_id = results.id\n",
    "\n",
    "# print('best result: ', best_loss)\n",
    "# print('best parameters: ', best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"run_tuning\"></a>\n",
    "## Run Prompt Tuning \n",
    "\n",
    "**Note**: This will take approx. 20-25 mins, if you wish you can skip this step, navigate to **Foundation Models Inference on watsonx.ai** and use our tuned/deployed model with provided credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run a prompt tuning process of foundation model on top of the training data referenced by DataConnection (tuning may take some time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define `prompt_tuner` parameters. In the example below the `learning_rate` value was established using the HPO runs described above.\n",
    "\n",
    "**Action:** For parameters details, see <a href=\"https://ibm.github.io/watsonx-ai-python-sdk/tune_experiment.html#ibm_watsonx_ai.experiment.fm_tune.TuneExperiment.prompt_tuner\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt_tuner = experiment.prompt_tuner(name=\"Llama-2-13b-chat prompt tuning - python SDK\",\n",
    "                                       task_id=experiment.Tasks.CLASSIFICATION,\n",
    "                                       base_model='google/flan-t5-xl',\n",
    "                                       accumulate_steps=32,\n",
    "                                       batch_size=8,\n",
    "                                       learning_rate=0.05881198993298603,\n",
    "                                       max_input_tokens=256,\n",
    "                                       max_output_tokens=10,\n",
    "                                       num_epochs=6,\n",
    "                                       tuning_type=experiment.PromptTuningTypes.PT,\n",
    "                                       verbalizer=\"Including narratives choice the best match product with the items from the list: 'credit_card', 'debt_collection', 'mortgages_and_loans', 'credit_reporting', 'retail_banking'. Input: {{input}} Output: \",\n",
    "                                       auto_update_model=True\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can review previous set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt_tuner.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By changing the `background_mode` parameter to `True`, the prompt tuning process will run in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "tuning_details = prompt_tuner.run(training_data_references=[data_conn], background_mode=True)\n",
    "print(f\"Starting the tune...\")\n",
    "while prompt_tuner.get_run_status() not in ['failed', 'canceled', 'completed']:\n",
    "    time.sleep(60)\n",
    "    try:\n",
    "        loss = prompt_tuner.summary().loss.values[0]\n",
    "    except:\n",
    "        loss = None\n",
    "    print(\"Loss: \", loss)\n",
    "end = time.time() - start\n",
    "try:\n",
    "    loss = prompt_tuner.summary().loss.values[0]\n",
    "except:\n",
    "    loss = 10\n",
    "print(prompt_tuner.get_run_status(), ', Loss: ',  loss, ' | Run time: ', end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you can view your tuning experiments by navigating to your project on watsonx.ai user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"run_details\"></a>\n",
    "## Prompt Tuning details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check status/state of initialized Prompt Tuning run if ran in background mode or when process finish if background mode is off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt_tuner.get_run_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's summarize the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt_tuner.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt_tuner.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"deploy\"></a>\n",
    "### Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can specify `model_id` from tuning details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_id = None\n",
    "\n",
    "if 'model_id' in tuning_details.get('entity', {}):\n",
    "    model_id = tuning_details['entity']['model_id']\n",
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create online deployment for published model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "meta_props = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"PT DEPLOYMENT SDK - project\",\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
    "    client.deployments.ConfigurationMetaNames.SERVING_NAME : f\"pt_sdk_deployment_{datetime.utcnow().strftime('%Y_%m_%d_%H%M%S')}\"\n",
    "}\n",
    "deployment_details = client.deployments.create(model_id, meta_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Additionally you can get deployment details by printing `deployment_details`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "deployment_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can specify `deployment_id` from deployment details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "deployment_id = deployment_details['metadata']['id']\n",
    "deployment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"models_inference\"></a>\n",
    "## Foundation Models Inference on watsonx.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Provide a set of model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally, if you want to use an already Tuned (and deployed model), use the provided credentials. Else, go ahead and tune your own model, it will take a while (approx. 20-25 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = {\n",
    "    'url': \"https://us-south.ml.cloud.ibm.com\",\n",
    "    'api_key': getpass.getpass(\"Please enter the corresponding api key (hit enter): \"),\n",
    "    'project_id': input(\"Please enter the corresponding project ID (hit enter): \"),\n",
    "    'deployment_id': input(\"Input the deployment_id (hit enter): \"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = creds[\"project_id\"]\n",
    "deployment_id = creds[\"deployment_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = APIClient(creds)\n",
    "client.set.default_project(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "generate_params = {\n",
    "    GenParams.MAX_NEW_TOKENS: 20,\n",
    "    GenParams.STOP_SEQUENCES: [\"\\n\\n\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Initialize the `ModelInference` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "\n",
    "tuned_model = ModelInference(\n",
    "    deployment_id=deployment_id,\n",
    "    params=generate_params,\n",
    "    api_client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Alternatively you can use `credentials` and `project_id` to initialize the `ModelInference` class.\n",
    "\n",
    "```\n",
    "tuned_model = ModelInference(\n",
    "    deployment_id=deployment_id,\n",
    "    params=generate_params,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Get deployment model inference details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuned_model.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Analyze the product class for a sample prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "response = tuned_model.generate_text(prompt=\"Including narratives choice the best match product with the items from the list: 'credit_card', 'debt_collection', 'mortgages_and_loans', 'credit_reporting', 'retail_banking'.\\nComment: hi landed job reside ca needed room rent found place rent paid deposit dollar however position going didnt work longer needed rent place bay asked landlord refund security deposit refused told called back wellsfargo disputed transaction recently noticed card reversal checking account got charged amount dollar called bank werent able refund money also emailed landlord asking refund money ten day passed still response hope cfpb take action successfully resolve issue thank\\nProduct:\\n\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"predict\"></a>\n",
    "## Analyze the product classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Download and prepare the `cfpb_test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = 'cfpb_test.json'\n",
    "url = \"https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/prompt_tuning/cfpb_test.json\"\n",
    "if not os.path.isfile(filename):\n",
    "    wget.download(url)\n",
    "    \n",
    "data = pd.read_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = list(data.input)\n",
    "products = list(data.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts_batch = [\"\\n\".join([prompt]) for prompt in prompts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate the accuracy of tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuned_model_results = tuned_model.generate_text(prompt=prompts_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's strip whitespaces from the generated text to have reliable accuracy calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuned_model_results = [x.strip() for x in tuned_model_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'accuracy_score: {accuracy_score(products, tuned_model_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate the accuracy of base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Initialize the `ModelInference` class with base model.\n",
    "\n",
    "**Note**: if you have tuned your own model, you need to make sure the `model_id` below matches the base model you have used for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model = ModelInference(\n",
    "    model_id='meta-llama/llama-2-13b-chat',\n",
    "    params=generate_params,\n",
    "    api_client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model_results = base_model.generate_text(prompt=prompts_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's strip whitespaces from the generated text to have reliable accuracy calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_results = [x.strip() for x in base_model_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'base model accuracy_score: {accuracy_score(products, base_model_results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary and next steps\n",
    "\n",
    " You successfully completed this notebook!.\n",
    " \n",
    " You learned how to use prompt tuning in watsonx for analyze the product classes for a sample prompts.\n",
    " \n",
    "Check out our _<a href=\"https://ibm.github.io/watson-machine-learning-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Authors: \n",
    "\n",
    "**Lukasz Cmielowski**, Senior Technical Staff Member at Watson Machine Learning,\n",
    "\n",
    "**Mateusz Szewczyk**, Software Engineer at Watson Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright © 2024 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook-samples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
