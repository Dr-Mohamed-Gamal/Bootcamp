{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Lab Challenge Exercises Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second prompt lab in the bootcamp series, you should have completed lab 1 and the exercises follow on from those. If you completed all the exercises in Lab 1 you should find most of the exercises here straightforward\n",
    "\n",
    "This notebook is a template with all the exercises and indications of what the output should look like if you do a good job with the prompts.\n",
    "\n",
    "Before you start you should have a Python environment with the necessary libraries installed as indicated in the intro lab, you will also need a .env file with: \n",
    "- your IBM Cloud API key\n",
    "- the IBM Cloud regional URL (eg, https://us-south.ml.cloud.ibm.com)\n",
    "- the project ID associated with your WatsonX project (required by the WML Python SDK)\n",
    "\n",
    "It should take you about 30-45 min to walk through the exercises self paced\n",
    "\n",
    "Good luck and make sure you compare your answers with the model solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"python-dotenv==1.0.0\" | tail -n 1\n",
    "!pip install \"ibm-watsonx-ai\" | tail -n 1\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "# List available models\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "print([model.name for model in ModelTypes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load credentials for Watsonx.ai (note refer to lab explaining how to do this if necessary)\n",
    "    - you should have a .env file with your IBM Cloud API key, eg API_KEY=xxx\n",
    "    - you should have a .env with the IBM Cloud regional url, eg IBM_CLOUD_URL=https://us-south.ml.cloud.ibm.com\n",
    "    - you should have a .env with the associated WatsonX project ID, eg PROJECT_ID=xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"API_KEY\")\n",
    "except Exception:\n",
    "    api_key = getpass.getpass(\"Please enter your api key (hit enter): \")\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "project_id = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for text generation with the [WML Python SDK](https://ibm.github.io/watson-machine-learning-sdk/foundation_models.html) for foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_watsonxai(prompts,\n",
    "                    model_name=\"google/flan-ul2\",\n",
    "                    decoding_method=\"greedy\",\n",
    "                    max_new_tokens=100,\n",
    "                    min_new_tokens=30,\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=1.0,\n",
    "                    stop_sequences=[\"..\"]\n",
    "                    ):\n",
    "    '''\n",
    "   helper function for sending prompts and params to Watsonx.ai\n",
    "    \n",
    "    Args:  \n",
    "        prompts:list list of text prompts\n",
    "        decoding:str Watsonx.ai parameter \"sample\" or \"greedy\"\n",
    "        max_new_tok:int Watsonx.ai parameter for max new tokens/response returned\n",
    "        temperature:float Watsonx.ai parameter for temperature (range 0>2)\n",
    "        repetition_penalty:float Watsonx.ai parameter for repetition penalty (range 1.0 to 2.0)\n",
    "\n",
    "    Returns: None\n",
    "        prints response\n",
    "    '''\n",
    "\n",
    "    assert not any(map(lambda prompt: len(prompt) < 1, prompts)), \"make sure none of the prompts in the inputs prompts are empty\"\n",
    "\n",
    "    # Instantiate parameters for text generation\n",
    "    model_params = {\n",
    "        GenParams.DECODING_METHOD: decoding_method,\n",
    "        GenParams.MIN_NEW_TOKENS: min_new_tokens,\n",
    "        GenParams.MAX_NEW_TOKENS: max_new_tokens,\n",
    "        GenParams.RANDOM_SEED: 42,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "        GenParams.REPETITION_PENALTY: repetition_penalty,\n",
    "        GenParams.STOP_SEQUENCES: stop_sequences\n",
    "    }\n",
    "\n",
    "\n",
    "    # Instantiate a model proxy object to send your requests\n",
    "    model = Model(\n",
    "        model_id=model_name,\n",
    "        params=model_params,\n",
    "        credentials=creds,\n",
    "        project_id=project_id)\n",
    "    #print(prompts)\n",
    "\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(model.generate_text(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_model = 'bigscience/mt0-xxl'\n",
    "flanul_model = 'google/flan-ul2'\n",
    "granite_13b_instruct_v2 = 'ibm/granite-13b-instruct-v2'\n",
    "llama2 = \"meta-llama/llama-2-70b-chat\"\n",
    "t5 = \"google/flan-t5-xxl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Review for Questions  1 - 5\n",
    "lamp_review = \"\"\"Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1) write a prompt to return the sentiment of the review\n",
    "Target sentiment = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1 Code - enter prompt and parameters in this cell\n",
    "prompt = f\"\"\"\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\" #Complete your prompt here \n",
    "\n",
    "send_to_watsonxai(prompts=[prompt],\n",
    "                  min_new_tokens= 1,\n",
    "                  max_new_tokens= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2) extract the emotions the reviewer expressed, return answer as a comma separated list\n",
    "Target emotions = satisfied, happy, cared for, great company, product!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\" #Complete your prompt here \n",
    "\n",
    "send_to_watsonxai(prompts=[prompt],min_new_tokens=10,max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3) Is the reviewer expressing anger, answer “yes” or “no” – test with your own example including anger to ensure it works in both cases.\n",
    "Target answer = no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\" #Complete your prompt here \n",
    "\n",
    "send_to_watsonxai(prompts=[prompt],max_new_tokens=2,min_new_tokens=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4) Extract the item purchased and the company name, return as JSON format\n",
    "Target answer = Item[lamp], Brand[Lumina]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Review text: ```{lamp_review}```\n",
    "\"\"\"#Complete your prompt here \n",
    "\n",
    "send_to_watsonxai(prompts=[prompt],max_new_tokens=30,min_new_tokens=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5) Can you combine 3-6 in a single prompt and return JSON with: Sentiment (negative or positive), Anger (yes/no), Product, Company\n",
    "Target answer = Sentiment[positive], Anger[false], Item[lamp], Brand[Lumina]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"#Complete your prompt here \n",
    "\n",
    "send_to_watsonxai(prompts=[prompt],max_new_tokens=50,min_new_tokens=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6) summarize the following product review\n",
    "Example summary = My daughter loves it!  It's soft and  super cute, and its face has a friendly look. It's  a bit small for what I paid though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Review for Questions  6 - 8\n",
    "review = \"\"\"Got this panda plush toy for my daughter's birthday, \\\n",
    "who loves it and takes it everywhere. It's soft and \\ \n",
    "super cute, and its face has a friendly look. It's \\ \n",
    "a bit small for what I paid though. I think there \\ \n",
    "might be other options that are bigger for the \\ \n",
    "same price. It arrived a day earlier than expected, \\ \n",
    "so I got to play with it myself before I gave it to her.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Review: ```{review}```\n",
    "\"\"\"#Complete your prompt here \n",
    "\n",
    "send_to_watsonxai(prompts=[prompt],max_new_tokens=50,min_new_tokens=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7) Summarize the same product review from the perspective of the shipping department\n",
    "Example summary = It arrived a day earlier than expected, so I got to play with it myself before I gave it  to her. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concise wrt feedback shipping\n",
    "prompt = f\"\"\"\n",
    "Review: ```{review}```\n",
    "\"\"\"#Complete your prompt here \n",
    "\n",
    "send_to_watsonxai(prompts=[prompt],max_new_tokens=50, min_new_tokens=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8) Summarize the review from the perspective of pricing and value\n",
    "Example summary = It's a bit small for what I paid though. I think there might be other options that are bigger for the same price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feedback pricing works - concise\n",
    "prompt = f\"\"\"\n",
    "Review: ```{review}```\n",
    "\"\"\"#Complete your prompt here \n",
    "\n",
    "send_to_watsonxai(prompts=[prompt],min_new_tokens=10,max_new_tokens=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9)\tPII removal. Given the following email, write a prompt to remove the PII (eg names, emails etc) (Hint: you may need to use 1-2 shot technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Hi John,\\\n",
    "\n",
    "I'm writing to you because I noticed you recently purchased a new car. I'm a salesperson\\\n",
    "at a local dealership (Cheap Dealz), and I wanted to let you know that we have a great deal on a new\\\n",
    "car. If you're interested, please let me know.\\\n",
    "\n",
    "Thanks,\\\n",
    "\n",
    "Jimmy Smith\\\n",
    "\n",
    "Phone: 410-805-2345\\\n",
    "Email: jimmysmith@cheapdealz.com\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint - use prompt template or manually construct the prompt with f strings (look up in documentation if unsure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Input:\n",
    "{email}\n",
    "\n",
    "Output:\n",
    "\"\"\"#Complete your prompt here \n",
    "response = send_to_watsonxai(prompts=[prompt], model_name=\"mistralai/mixtral-8x7b-instruct-v01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10) Basic inference: A patients a1c level determines their diabetes status, the rules are as follows:\n",
    "\n",
    " - less than 5.7 no diabetes\n",
    " - between 5.7 and 6.5 pre-diabetes\n",
    " - greater than 6.5 diabetic.\n",
    "\n",
    "Write a prompt to return just the diabetes status from the following 3 test cases:\n",
    "\n",
    "1)\tThe patients a1c is 5.5 which is good considering his other risk factors.\n",
    "2)\tFrom the last lab report I noted the A1c is 6.4 so we need to put her on Ozempic.\n",
    "3)\tShe mentioned her A1c is 8 according to her blood work about 3 years ago.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record1 = \"The patients a1c is 5.5 which is good considering his other risk factors.\"\n",
    "record2 = \"From the last lab report I noted the A1c is 6.4 so we need to put her on Ozempic.\"\n",
    "record3 = \"She mentioned her A1c is 8 according to her blood work about 3 years ago.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1 ENTER YOUR MODEL PARAMS HERE - MAKE SURE IT WORKS WITH ALL 3 EXAMPLES ABOVE\n",
    "prompt = f\"\"\"\n",
    "```{record1}```\n",
    "\"\"\"#Complete your prompt here \n",
    "\n",
    "send_to_watsonxai(prompts=[prompt],max_new_tokens=10,min_new_tokens=1,model_name=\"mistralai/mixtral-8x7b-instruct-v01\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
