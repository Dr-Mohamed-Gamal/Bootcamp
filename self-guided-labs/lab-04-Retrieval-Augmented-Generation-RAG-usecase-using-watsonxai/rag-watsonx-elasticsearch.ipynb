{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "Use watsonx, and Elasticsearch Python SDK to answer questions (RAG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimers\n",
    "\n",
    "- Use only Projects and Spaces that are available in watsonx context.\n",
    "\n",
    "# Notebook content\n",
    "\n",
    "This notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation in watsonx.ai. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.10.\n",
    "\n",
    "### About Retrieval Augmented Generation\n",
    "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
    "\n",
    "In its simplest form, RAG requires 3 steps:\n",
    "\n",
    "- Index knowledge base passages (once)\n",
    "- Retrieve relevant passage(s) from knowledge base (for every user query)\n",
    "- Generate a response by feeding retrieved passage into a large language model (for every user query)\n",
    "\n",
    "# Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Data (test) loading](#data)\n",
    "- [Foundation Models on watsonx](#models)\n",
    "- [Basic information how to connect to Elasticsearch (applies to both scenarios)](#elastic_conn)\n",
    "- **[Retrieval augmented generation using Elasticsearch (Python Client)](#elastic)**\n",
    "    - [Create index](#mapping)\n",
    "    - [Index data into Elasticsearch](#index_data)\n",
    "    - [Run semantic knn search queries](#knn)\n",
    "    - [Calculate rougeL metric](#elastic_score)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"langchain==0.0.340\" | tail -n 1\n",
    "!pip install elasticsearch | tail -n 1\n",
    "!pip install sentence_transformers | tail -n 1\n",
    "!pip install pandas | tail -n 1\n",
    "!pip install rouge_score | tail -n 1\n",
    "!pip install nltk | tail -n 1\n",
    "!pip install wget | tail -n 1\n",
    "!pip install evaluate | tail -n 1\n",
    "!pip install \"pydantic==1.10.0\" | tail -n 1\n",
    "!pip install \"ibm-watsonx-ai>=0.1.2\" | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## watsonx API connection\n",
    "This cell defines the credentials required to work with watsonx API for Foundation\n",
    "Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"API_KEY\")\n",
    "except Exception:\n",
    "    api_key = getpass.getpass(\"Please enter your api key (hit enter): \")\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "project_id = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "# Data (test) loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the test dataset. This dataset is used to calculate the metrics score for selected model, defined prompts and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "questions_test_filename = 'questions_test.csv'\n",
    "questions_train_filename = 'questions_train.csv'\n",
    "questions_test_url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/RAG/questions_test.csv'\n",
    "questions_train_url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/RAG/questions_train.csv'\n",
    "\n",
    "\n",
    "if not os.path.isfile(questions_test_filename): \n",
    "    wget.download(questions_test_url, out=questions_test_filename)\n",
    "\n",
    "\n",
    "if not os.path.isfile(questions_train_filename): \n",
    "    wget.download(questions_train_url, out=questions_train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_test = './questions_test.csv'\n",
    "filename_train =  './questions_train.csv'\n",
    "\n",
    "test_data = pd.read_csv(filename_test)\n",
    "train_data = pd.read_csv(filename_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up knowledge base\n",
    "\n",
    "The current state-of-the-art in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
    "\n",
    "We can generate dense vector representations using embedding models. In this notebook, we use <a href=\"https://www.sbert.net/\" target=\"_blank\" rel=\"noopener no referrer\">SentenceTransformers</a> <a href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\" target=\"_blank\" rel=\"noopener no referrer\">all-MiniLM-L6-v2</a> to embed both the knowledge base passages and user queries. `all-MiniLM-L6-v2` is a performant open-source model that is small enough to run locally.\n",
    "\n",
    "A vector database is optimized for dense vector indexing and retrieval. This notebook uses <a href=\"https://python.langchain.com/docs/integrations/vectorstores/elasticsearch#basic-example\" target=\"_blank\" rel=\"noopener no referrer\">Elasticsearch</a>, a distributed, RESTful search and analytics engine, capable of performing both vector and lexical search. It is built on top of the Apache Lucene library, which offers good speed and performance with all-MiniLM-L6-v2 embedding model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using is already split into self-contained passages that can be ingested by Elasticsearch. \n",
    "\n",
    "The size of each passage is limited by the embedding model's context window (which is 256 tokens for `all-MiniLM-L6-v2`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load knowledge base documents\n",
    "\n",
    "Load set of documents used further to build knowledge base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_dir = \"./knowledge_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = f\"{os.getcwd()}/knowledge_base\"\n",
    "if not os.path.isdir(my_path):\n",
    "   os.makedirs(my_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_filename = 'knowledge_base/psgs.tsv'\n",
    "documents_url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/RAG/psgs.tsv'\n",
    "\n",
    "\n",
    "if not os.path.isfile(documents_filename): \n",
    "    wget.download(documents_url, out=documents_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.read_csv(f\"{knowledge_base_dir}/psgs.tsv\", sep='\\t', header=0)\n",
    "documents['indextext'] = documents['title'].astype(str) + \"\\n\" + documents['text']\n",
    "documents = documents[:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an embedding function\n",
    "\n",
    "Note that you can feed a custom embedding function to be used by Elasticsearch. The performance of Elasticsearch may differ depending on the embedding model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "emb_func = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "# Foundation Models on watsonx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model\n",
    "You need to specify `model_id` that will be used for inferencing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "print([model.name for model in ModelTypes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "model_id = ModelTypes.MIXTRAL_8X7B_INSTRUCT_V01_Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model parameters\n",
    "We need to provide a set of model parameters that will influence the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `ModelInference` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "\n",
    "model = ModelInference(\n",
    "    model_id=model_id,\n",
    "    params=parameters,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"elastic_conn\"></a>\n",
    "# Basic information how to connect to Elasticsearch \n",
    "\n",
    "**This notebook focuses on self-managed cluster using <a href=\"https://cloud.ibm.com/docs/databases-for-elasticsearch?topic=databases-for-elasticsearch-getting-started\" target=\"_blank\" rel=\"noopener no referrer\">IBM Cloud® Databases for Elasticsearch.</a>**\n",
    "\n",
    "The following cell retrieves the Elasticsearch users, password, host and port from the environment if available and prompts you otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "elastic_client = Elasticsearch(\n",
    "    hosts = [os.getenv('ES_URL')],\n",
    "    verify_certs=False,\n",
    "    api_key=os.getenv('ES_API_KEY'),\n",
    "    request_timeout=None\n",
    ")\n",
    "\n",
    "elastic_client.health_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"elastic\"></a>\n",
    "Retrieval augmented generation using Elasticsearch (Python SDK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario the same embedding function `all-MiniLM-L6-v2` will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = emb_func.client.get_sentence_embedding_dimension()\n",
    "dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mapping\"></a>\n",
    "## Create index\n",
    "To create Elasticsearch index necessary mappings need to be created. This will enable index the data into Elasticsearch.\n",
    "\n",
    "Field `dense_vector` is a special type that allows to store dense vectors in this case `embedding` in Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "index_name = \"elastic_knn_index\"\n",
    "index_name = f\"{index_name}_{str(uuid.uuid4())[:5]}\"\n",
    "\n",
    "mapping = {\n",
    "        \"properties\": {\n",
    "                \"text\": {\n",
    "                        \"type\": \"text\"\n",
    "                    },\n",
    "                \"embedding\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": dims,\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"l2_norm\"\n",
    "                    }\n",
    "            }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if elastic_client.indices.exists(index=index_name):\n",
    "    elastic_client.indices.delete(index=index_name)\n",
    "    \n",
    "elastic_client.indices.create(index=index_name, mappings=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"index_data\"></a>\n",
    "## Index data into Elasticsearch\n",
    "\n",
    "The following function generates the required bulk actions that can be passed to Elasticsearch's Bulk API, so we can index multiple documents efficiently. To perform semantic search, we need to encode queries with the same embedding model used to encode the documents at index time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents.indextext.tolist()\n",
    "embedded_docs = emb_func.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "document_list = []\n",
    "batch_size=500\n",
    "requests = []\n",
    "for i, (text, vector) in enumerate(zip(texts, embedded_docs)):\n",
    "    document = {\"_id\": i, \"embedding\": vector, 'text': text}\n",
    "    document_list.append(document)\n",
    "    if i % batch_size == batch_size-1:\n",
    "        success, failed = bulk(elastic_client, document_list, index=index_name)\n",
    "        document_list = []\n",
    "\n",
    "elastic_client.indices.refresh(index=index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select questions\n",
    "\n",
    "Get questions from the previously loaded test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_and_answers = [\n",
    "            ('names of founding fathers of the united states?', \"Thomas Jefferson::James Madison::John Jay::George Washington::John Adams::Benjamin Franklin::Alexander Hamilton\"),\n",
    "            ('who played in the super bowl in 2013?', 'Baltimore Ravens::San Francisco 49ers'),\n",
    "            ('when did bucharest become the capital of romania?', '1862')\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn\"></a>\n",
    "# Run semantic search queries\n",
    "\n",
    "Now it's time to run queries against our Elasticsearch index using our encoded question. We'll be doing a k-nearest neighbors search, using the Elasticsearch kNN query option. Argument k stands for a number of nearest neighbors to return as top hits. Set minimal similarity score to 0.45 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_contexts = []\n",
    "\n",
    "for question_text, _ in questions_and_answers:\n",
    "    embedded_question = emb_func.embed_query(question_text)\n",
    "    relevant_chunks = elastic_client.search(\n",
    "          index=index_name,\n",
    "          knn={\n",
    "            \"field\": \"embedding\",\n",
    "            \"query_vector\": embedded_question,\n",
    "            \"k\": 4,\n",
    "            \"num_candidates\": 50,\n",
    "            },\n",
    "          _source=[\n",
    "                    \"text\"\n",
    "                  ]                       \n",
    "    )\n",
    "    relevant_contexts.append(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_context = relevant_contexts[0]\n",
    "hits = relevant_context['hits']['hits']\n",
    "for hit in hits:\n",
    "    print(\"=========\")\n",
    "    print(\"Paragraph index : \", hit[\"_id\"])\n",
    "    print(\"Paragraph : \", hit[\"_source\"]['text'])\n",
    "    print(\"Distance : \",  hit[\"_score\"])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed the context and the questions to `watsonx.ai` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(context, question_text):\n",
    "    return (f\"Please answer the following.\\n\"\n",
    "          + f\"{context}:\\n\\n\"\n",
    "          + f\"{question_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_texts = []\n",
    "\n",
    "for relevant_context, (question_text, _) in zip(relevant_contexts, questions_and_answers):\n",
    "    hits = [hit for hit in relevant_context[\"hits\"][\"hits\"]]\n",
    "    context = \"\\n\\n\\n\".join([rel_ctx[\"_source\"]['text'] for rel_ctx in hits])\n",
    "    prompt_text = make_prompt(context, question_text)\n",
    "    prompt_texts.append(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a retrieval-augmented response with watsonx.ai model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for prompt_text in prompt_texts:\n",
    "    results.append(model.generate_text(prompt_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, result in enumerate(results):\n",
    "    print('*******************************************')\n",
    "    print(\"Question = \", questions_and_answers[idx][0])\n",
    "    print(\"Answer = \", result)\n",
    "    print(\">>>> Expected Answer(s) (may not be appear with exact wording in the dataset) = \",  questions_and_answers[idx][1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"score\"></a>\n",
    "# Calculate rougeL metric \n",
    "In this sample notebook `evaluate` module from HuggingFace was used for rougeL calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load('rouge')\n",
    "scores = rouge.compute(predictions=results, references=[answer for _, answer in questions_and_answers])\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2023, 2024 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIG-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
