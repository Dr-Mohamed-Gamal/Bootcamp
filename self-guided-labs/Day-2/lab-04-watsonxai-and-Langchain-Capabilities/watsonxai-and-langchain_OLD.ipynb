{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c203bbd",
   "metadata": {},
   "source": [
    "# Lab 4: Using LangChain with IBM WatsonX\n",
    "\n",
    "## 1. Intro to LangChain\n",
    "\n",
    "[LangChain](https://docs.langchain.com/docs/) is an open-source development framework designed to simplify the creation of applications using large language models (LLMs).\n",
    "\n",
    "The core idea of the library is that we can \"chain\" together different components to create more advanced use cases around LLMs. Here are the main components for the LangChain\n",
    "\n",
    "- Model: interact with various LLMs\n",
    "- Prompts: text that is sent to the LLMs\n",
    "- Chains: allow to combine different LLM calls and actions automatically\n",
    "- Embeddings and Vector Stores: break large data into chunks and store those to be queried when relevant\n",
    "- Agents: enbale the LLMs to dynamically decide which tools to use in order to best respond to a given query\n",
    "\n",
    "In short, **Langchain is a framework that can orchestrate a series of prompts to achieve a desired outcomes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c30de",
   "metadata": {},
   "source": [
    "## 2. How to connect LangChain to WatsonX.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4adcdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, List, Mapping, Optional, Union, Dict\n",
    "from pydantic import BaseModel, Extra\n",
    "try:\n",
    "    from langchain import PromptTemplate\n",
    "    from langchain.document_loaders import WebBaseLoader\n",
    "    from langchain.chains.summarize import load_summarize_chain\n",
    "    from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "    from langchain.chains.llm import LLMChain\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    from langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace embedding models\n",
    "    from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
    "    from langchain.llms.base import LLM\n",
    "    from langchain.llms.utils import enforce_stop_tokens\n",
    "except ImportError:\n",
    "    raise ImportError(\"Could not import langchain: Please install ibm-generative-ai[langchain] extension.\")\n",
    "\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96339420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config Watsonx.ai environment\n",
    "\n",
    "api_key = \"API_KEY\"\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "project_id = \"PROJECT_ID\"\n",
    "\n",
    "\n",
    "creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51cbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "##initializing WatsonX model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.RANDOM_SEED: 42,\n",
    "    GenParams.TEMPERATURE: 0.5,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P:1\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    model_id='google/flan-ul2',\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e37376-3f85-4751-9aba-9550af6cbf37",
   "metadata": {},
   "source": [
    "In order to use WatsonX-based LLMs with Langchain, the LLM object must be of class `BaseLanguageModel` (see [Langchain docs](https://api.python.langchain.com/en/latest/schema/langchain.schema.language_model.BaseLanguageModel.html)). We'll use the custom class below to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805b7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the WatsonX Model in a langchain.llms.base.LLM subclass to allow LangChain to interact with the model\n",
    "\n",
    "class LangChainInterface(LLM, BaseModel):\n",
    "    credentials: Optional[Dict] = None\n",
    "    model: Optional[str] = None\n",
    "    params: Optional[Dict] = None\n",
    "    project_id : Optional[str]=None\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        _params = self.params or {}\n",
    "        return {\n",
    "            **{\"model\": self.model},\n",
    "            **{\"params\": _params},\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"IBM WATSONX\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Call the WatsonX model\"\"\"\n",
    "        params = self.params or {}\n",
    "        model = Model(model_id=self.model, params=params, credentials=self.credentials, project_id=self.project_id)\n",
    "        text = model.generate_text(prompt)\n",
    "        if stop is not None:\n",
    "            text = enforce_stop_tokens(text, stop)\n",
    "        return text\n",
    "\n",
    "llm_model = LangChainInterface(model='google/flan-ul2', credentials=creds, params=params, project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b25c008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seoul'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##predict with the model\n",
    "text = \"Where is the capital of South Korea\"\n",
    "llm_model(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e2e8f",
   "metadata": {},
   "source": [
    "## 3. Prompt Templates & Chains\n",
    "\n",
    "In the previous example, the user input is sent directly to the LLM. However, when using an LLM in an application, you will usually need to reuse the same prompt across multiple scenarios\n",
    "\n",
    "- Accepting user input and contruct a prompt\n",
    "- Generating mutiple prompts from an collection of data points in a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "310c2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the capital of USA? = washington dc\n",
      "where is the capital of England? = london\n",
      "where is the capital of Japan? = tokyo\n",
      "where is the capital of Saudi Arabia? = jeddah\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt templates\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"country\"],\n",
    "  template= \"where is the capital of {country}?\",\n",
    ")\n",
    "\n",
    "# Chaining \n",
    "chain = LLMChain(llm=llm_model, prompt=prompt)\n",
    "\n",
    "# Getting predictions\n",
    "countries = [\"USA\", \"England\", \"Japan\", \"Saudi Arabia\"]\n",
    "for country in countries:\n",
    "    response = chain.run(country)\n",
    "    print(prompt.format(country=country) + \" = \" + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a9df3",
   "metadata": {},
   "source": [
    "## 4. Simple sequential chains\n",
    "The utility of LangChain becomes apparent as we chain outputs of one model as input to another model. Here's a simple example where one generates a question which the other model answers.\n",
    "\n",
    "LangChain determines a model's output based on its response.  In our examples, the first model creates a response to the end prompt of \"Question:\" which LangChain maps as an input variable called \"question\" which it passes to the 2nd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffda7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create two sequential prompts \n",
    "pt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\n",
    "pt2 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23e4e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan = LangChainInterface(model='google/flan-ul2', credentials=creds, params=params, project_id=project_id)\n",
    "model = LangChainInterface(model='google/flan-ul2', credentials=creds, project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35de1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_flan = LLMChain(llm=flan, prompt=pt1)\n",
    "flan_to_model = LLMChain(llm=model, prompt=pt2)\n",
    "qa = SimpleSequentialChain(chains=[prompt_to_flan, flan_to_model], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34586549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mWhat does the term \"artificial intelligence\" mean?\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mintelligent computer program\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'intelligent computer program'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"artificial intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344acaa",
   "metadata": {},
   "source": [
    "## 5. Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57e3116d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading web document...\n",
      "Done.\n",
      "Initializing mixtral-8x7b model...\n",
      "Initializing chain...\n",
      "Stuff chain with documents...\n",
      "Running summarization on stuffed document chain...\n",
      "\n",
      "\n",
      "\n",
      "IBM helped a multinational automobile manufacturer reduce defects and downtime in their manufacturing process by deploying AI-enabled automated inspections. The solutions included fixed-mounted, handheld, and wearable inspections, which were based on standard iPhones and used readily available hardware. The lightweight and portable nature of IBM's solution, combined with its ability to be used anywhere at any time, was a major selling point for the client. The IBM Inspection Suite improved the client's quality inspection process without requiring coding and was simple to train and deploy. The system learned quickly from images of acceptable and defective work products, enabling it to be up and running within a matter of weeks. The implementation costs were also lower than those of viable alternatives. The ability to deliver AI-enabled automation by using an intuitive process in their plants allowed this client to scale this user-friendly technology rapidly across numerous other facilities where it aided in over 30 million ins\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Initialize llm and document loader:\n",
    "print(\"Loading web document...\")\n",
    "# Try out some other documents as well\n",
    "loader = WebBaseLoader(\"https://www.ibm.com/blog/reducing-defects-and-downtime-with-ai-enabled-automated-inspections/\")\n",
    "doc = loader.load()\n",
    "print(\"Done.\")\n",
    "\n",
    "# You might need to tweak some of the runtime parameters to optimize the results.\n",
    "print(\"Initializing mixtral-8x7b model...\")\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.15,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 20,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 20,\n",
    "    GenParams.MAX_NEW_TOKENS: 205,\n",
    "    GenParams.STOP_SEQUENCES: [\"\\n\"]\n",
    "}\n",
    "\n",
    "mixtral_model = Model(\n",
    "    model_id=\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following article in one paragraph:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "print(\"Initializing chain...\")\n",
    "llm_chain = LLMChain(llm=mixtral_model, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "print(\"Stuff chain with documents...\")\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
    ")\n",
    "\n",
    "print(\"Running summarization on stuffed document chain...\\n\")\n",
    "res = stuff_chain.run(doc)\n",
    "\n",
    "print(res)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
